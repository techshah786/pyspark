
Step-1: Write the code in the pycharm development environment. For example, the file name is "wordCount.py" and the code is below in the file.

import pyspark
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession


import sys
# import sys is used to input data where data set is stored like hdfs
# and output data where data set will be stored after transformation

if __name__ == "__main__":
    # Create SparkSession
    spark = SparkSession.builder \
        .appName("Pyspark Word Count") \
        .getOrCreate()

    sc = spark.sparkContext

    sourcepath = sys.argv[1]
    destinationpath = sys.argv[2]

    words = sc.textFile(sourcepath).flatMap(lambda line: line.split(" "))
    wordCount = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

    wordCount.saveAsTextFile(destinationpath)

    # Stop the SparkContext
    sc.stop()


    Step-2: Run Cloudera and in the cmd type "ifconfig" that will provide you IP address

    Step-3: Open winSCP (note: winSCP is app) and confige the setting to connect cloudera or hdfs (hadoop distribution file system). Just drug the .py file to hdfs.

    Step-4. Open putty, type "hdfs dfs -lrt |trail" and enter. You can see the .py is present in hdfs.
            and create a textFile named "wordCount.txt" by typing in putty "vi wordCount.txt" and writing two or three lines. and Save and exit the file

         5. Note: When we create a file, this is stored in the local system. We need to keep the created file in hdfs. We can do that in the following way.
                  hdfs dfs -put wordCount.txt /user/Cloudera
            Now, the file is inside hdfs

          6. Note: If you get an error, change the " safe mode off" using the following command: hdfs dfsadmin -safemode leave
         
         7. spark-submit: There are three modes to submit the spark-submit. For Example local, yarn client, and yarn cluster

           a. In local, processing and scheduling are taken care of by Spark. local mode is used for testing purposes.
           The spark-submit syntax: spark-submit --master local[*] .py_file sourcePath destinationPath. For example:
           
            spark-submit --master local[*] wordCount.py /user/cloudera/wordCount.txt  /user/cloudera/destination_path

            Note: We can see the output using the following command: hdfs dfs -cat /user/cloudera/destination_path/part=00000

            b. In yarn client, the processing part is taken care of by spark, and the scheduling part is taken care of by yarn. The yarn
            client mode is used for short-running jobs and this mode is used for a Development environment. The spark-submit is like the one below.

            spark-submit --master yarn --deploy-mode client wordCount.py /user/cloudera/wordCount.txt  /user/cloudera/destination_path

            c. In the yarn cluster, the spark-submit is done for the production environment and it is long running job. The syntax for the yarn cluster is below.

             spark-submit --master yarn --deploy-mode cluster wordCount.py /user/cloudera/wordCount.txt  /user/cloudera/destination_path

        8. Note: If we need to allocate resources, for example:
            How many executors do we need? 
            For each executor, how much memory is required?
            How many cores are required for each executor?

            we can do the following way the spark-submit:


        spark-submit --master yarn 
        --driver-cores 2
        --driver-memory 1G
        --num-executors 2
        --executor-cores 2
        --executor-memory 1G
        wordCount.py /user/cloudera/wordCount.txt  /user/cloudera/destination_path

    Note: The above syntax must be typed in one line, and this is a static allocation of resources. In product, the static allocation of resources is not applied.
        because of the huge volume of data.

        9. Dynamic Resource Allocation:

            spark-submit --master yarn --deploy-mode cluster
            --driver-memory 1GB
            --driver-core 4
            --executor-memory 2GB
            --executor cores 5
            --conf "spark.dynamicAllocation.enabled=true"
            --conf "spark.dynamicAllocation.minExecutors = 1"
            --conf "spark.dynamicAllocation.maxExecutors = 100"
            --conf "spark.shuffle.service.enabled=true"
             wordCount.py /user/cloudera/wordCount.txt  /user/cloudera/destination_path




**Interview Question**
Scenario-1: You are given cluster configuration. For spark-submit, you need to calculate how many
            --num-executor
            --executor-cores
            --executor-memory
        are required.

Scenario-2: Given 100GB data. Calculate how many
            --num-executor
            --executor-cores
            --executor-memory
        are required.

          
