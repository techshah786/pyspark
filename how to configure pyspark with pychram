Steps: How to configure spark in the system environment

1. Download spark on the local systems

2. Go to the system environment and select User Variable

3. Give Variable name like "SPARK_HOME" and in the Variable Value: paste location where you download the spark for example
  D:\pyCharm\spark-3.1.3-bin-hadoop3.2

4. Click on apply and ok

5. Select Path in the User Variable and click on edit

6. Select New and give the path like %SPARK_HOME%\bin

  







=============================================================================================================================================================
Steps: How to configure spark in pychram project

1. Create a project in pychram

2. Create a package 

3. Keep cursope on project and click on file and select setting

4. Find the project name and click on the project and click on Project Structure

5. On the right side, you will see "Add Content Root" and click on that

6. Find "spark-3.1.3-bin-hadoop3.2" where you downloaded and stored, for example: D:\PyCharm\spark-3.1.3-bin-hadoop3.2\python\lib\pyspark.zip

7. Select Python/lib/ and there you will see "py4j-0.10.9-src.zip" and "pyspark.zip" and select those two and click on apply and ok


